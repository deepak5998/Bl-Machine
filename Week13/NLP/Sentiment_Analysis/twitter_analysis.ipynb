{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# tokenizing in various ways\n",
    "from nltk.tokenize import line_tokenize, sent_tokenize, WordPunctTokenizer, word_tokenize\n",
    "# stopwwords collection\n",
    "from nltk.corpus import stopwords\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# to save and load models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "# punction sentence tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "# lemmatize like stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# to ramdomly shuffle\n",
    "import random\n",
    "# collection of dictionary, lemma ,examples. \n",
    "from nltk.corpus import wordnet \n",
    "# To Wrap up the sklearn classifiers to be used in NLP for classifying\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import time\n",
    "\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_file = open('Data/positive.txt','r').read()\n",
    "neg_file = open('Data/negetive.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_without_stopwords = [word for word in pos_file.split() if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for sentence in pos_file.splitlines():\n",
    "    doc.append((sentence,'pos'))\n",
    "for sentence in neg_file.splitlines():\n",
    "    doc.append((sentence,'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class by_count_vectorizer:\n",
    "\n",
    "    def __init__(self,doc):\n",
    "        self.doc = doc\n",
    "            \n",
    "    def feature_making(self):\n",
    "        X_train=[]\n",
    "        y_train=[]\n",
    "        for sent,cat in self.doc:\n",
    "            X_train.append(sent)\n",
    "            y_train.append(cat)\n",
    "        return X_train,y_train\n",
    "    \n",
    "    def convert_y_int(self,y_train,var):\n",
    "        for row in range(0,len(y_train)):\n",
    "            if y_train[row]==var:\n",
    "                 y_train[row]= 0\n",
    "            else:\n",
    "                y_train[row] = 1\n",
    "        return y_train\n",
    "    \n",
    "    def plotter(self,cross_val,classifier):\n",
    "                     \n",
    "        xar =[]\n",
    "        yar= []\n",
    "        \n",
    "        x=y=0\n",
    "        for rows in cross_val:\n",
    "            x += 1\n",
    "            print(x)\n",
    "            if classifier.predict(rows)==1:\n",
    "                y+=1\n",
    "            else:\n",
    "                y-=1\n",
    "            xar.append(x)\n",
    "            yar.append(y)\n",
    "                    \n",
    "            ax1.clear()\n",
    "            ax1.plot(xar,yar,color='green')\n",
    "            plt.show()\n",
    "            time.sleep(100)\n",
    "#             animation.FuncAnimation(fig,self.plotter,interval=1000)\n",
    "            \n",
    "    def classify_demo(self):\n",
    "        cv = CountVectorizer()\n",
    "        X_train,y_train = self.feature_making()\n",
    "        y_train = self.convert_y_int(y_train,'neg')\n",
    "        X = cv.fit_transform(X_train)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y_train,test_size=0.1)\n",
    "        x_train,x_cross,y_train,y_cross = train_test_split(x_train,y_train,test_size=0.2)\n",
    "        mnb = MultinomialNB()\n",
    "        mnb.fit(x_train,y_train)\n",
    "        y_pred = mnb.predict(x_cross)\n",
    "        print(\"Accuracy by f1 score \", f1_score(y_cross,y_pred)*100)\n",
    "        if(f1_score(y_cross,y_pred)>0.8):\n",
    "            feature_make_file = open('model_pickle/Count_vectorizer/count_vect.pkl','wb')\n",
    "            pickle.dump(cv,feature_make_file)\n",
    "            feature_make_file.close()\n",
    "            model_saver = open('model_pickle/Count_vectorizer/classifier.pkl','wb')\n",
    "            pickle.dump(mnb,feature_make_file)\n",
    "            model_saver.close()\n",
    "        feature = cv.transform([\"The movie was good and I loved it\"])\n",
    "        if(mnb.predict(feature)==1):\n",
    "            print('positive')\n",
    "        else:\n",
    "            print('negetive')\n",
    "        fig = plt.figure()\n",
    "        ax1 = self.fig.add_subplot(1,1,1)\n",
    "#         self.plotter(x_cross,mnb)\n",
    "        ani = animation.FuncAnimation(self.fig,self.plotter,interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_classifier = by_count_vectorizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by f1 score  77.5257731958763\n",
      "positive\n",
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ax1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-2912ff08ed00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-170-8b0d10697800>\u001b[0m in \u001b[0;36mclassify_demo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cross\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mani\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-170-8b0d10697800>\u001b[0m in \u001b[0;36mplotter\u001b[0;34m(self, cross_val, classifier)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0myar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ax1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_classifier.classify_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class by_tfid_vectorizer:\n",
    "\n",
    "    def __init__(self,doc):\n",
    "        self.doc = doc\n",
    "    \n",
    "    def feature_making(self):\n",
    "        X_train=[]\n",
    "        y_train=[]\n",
    "        for sent,cat in self.doc:\n",
    "            X_train.append(sent)\n",
    "            y_train.append(cat)\n",
    "        return X_train,y_train\n",
    "    \n",
    "    def convert_y_int(self,y_train,var):\n",
    "        for row in range(0,len(y_train)):\n",
    "            if y_train[row]==var:\n",
    "                 y_train[row]= 0\n",
    "            else:\n",
    "                y_train[row] = 1\n",
    "        return y_train\n",
    "    \n",
    "    def classify_demo(self):\n",
    "        cv = TfidfVectorizer(min_df=1,stop_words='english')\n",
    "        X_train,y_train = self.feature_making()\n",
    "        y_train = self.convert_y_int(y_train,'neg')\n",
    "        X = cv.fit_transform(X_train)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y_train,test_size=0.1)\n",
    "        x_train,x_cross,y_train,y_cross = train_test_split(x_train,y_train,test_size=0.2)\n",
    "        mnb = MultinomialNB()\n",
    "        mnb.fit(x_train,y_train)\n",
    "        y_pred = mnb.predict(x_cross)\n",
    "        print(\"Accuracy by f1 score {} and by accuracy_score is {}\".format(f1_score(y_cross,y_pred)*100,accuracy_score(y_cross,y_pred)*100))\n",
    "        if(f1_score(y_cross,y_pred)>0.8):\n",
    "            feature_make_file = open('model_pickle/Count_vectorizer/tfid_vectorizer.pkl','wb')\n",
    "            pickle.dump(cv,feature_make_file)\n",
    "            feature_make_file.close()\n",
    "            model_saver = open('model_pickle/Count_vectorizer/tfid_mnb_classifier.pkl','wb')\n",
    "            pickle.dump(mnb,feature_make_file)\n",
    "            model_saver.close()\n",
    "        feature = cv.transform([\"The movie was good and I loved it\"])\n",
    "        if(mnb.predict(feature)==1):\n",
    "            print('positive')\n",
    "        else:\n",
    "            print('negetive')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classify = by_tfid_vectorizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by f1 score 78.92331132554595 and by accuracy_score is 78.38541666666666\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "tf_classify.classify_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_cross,y_train,y_cross = train_test_split(x_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(x_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184639663335088"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cross,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192708333333334"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_cross,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking stopwords \n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "lem = WordNetLemmatizer()\n",
    "for words in word_tokenize(pos_file):\n",
    "    if words not in set(stopwords.words('english')):\n",
    "        all_words.append(lem.lemmatize(words.lower()))\n",
    "for words in word_tokenize(neg_file):\n",
    "    if words not in set(stopwords.words('english')):\n",
    "        all_words.append(lem.lemmatize(words.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "for sent in pos_file.splitlines():\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    tagged.append(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in neg_file.splitlines():\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    tagged.append(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_sent_tokenizer = PunktSentenceTokenizer(pos_file)\n",
    "# token = custom_sent_tokenizer.tokenize(pos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to take out noun, prepositions etc\n",
    "# chunked= []\n",
    "# for tok in token:\n",
    "#     words = nltk.word_tokenize(tok)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "#     chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "#     chunkParser = nltk.RegexpParser(chunkGram)\n",
    "#     chunked.append(chunkParser.parse(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_sent_tokenizer = PunktSentenceTokenizer(neg_file)\n",
    "# token = custom_sent_tokenizer.tokenize(neg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tok in token:\n",
    "#     words = nltk.word_tokenize(tok)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "#     chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "#     chunkParser = nltk.RegexpParser(chunkGram)\n",
    "#     chunked.append(chunkParser.parse(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(words_freq.keys())[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('romance' in top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_maker:\n",
    "    def __init__(self,top_words):\n",
    "        self.top_words = top_words\n",
    "        \n",
    "    def find_feature(self,doc):\n",
    "        features = {}\n",
    "        words = set(doc)\n",
    "        for w in self.top_words:\n",
    "            features[w] = (w in words)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = feature_maker(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ (fm.find_feature(sent),category) for (sent,category) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_make_file = open('model_pickle/feature_maker.pkl','wb')\n",
    "pickle.dump(fm,feature_make_file)\n",
    "feature_make_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = int(len(features)*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = features[:set1]\n",
    "test_set = features[set1:set1+5]\n",
    "cross_val = features[set1+5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('Data/test_set.pkl','wb')\n",
    "pickle.dump(test_set,test_file)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.NaiveBayesClassifier.train(cross_val)\n",
    "print(\"The accuracy for Naive bayes calssifier is\",nltk.classify.accuracy(classifier,cross_val)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open('model_pickle/classifier.pkl','wb')\n",
    "pickle.dump(classifier,model_file)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = fm.find_feature(\"The movie was bad no action and made no sence, it was not good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1641aa09bd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.classify(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
